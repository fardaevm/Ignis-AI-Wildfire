{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63581490",
   "metadata": {},
   "source": [
    "# Logistic Regression Baseline for Wildfire Prediction\n",
    "\n",
    "This notebook demonstrates how to build a **logistic regression** model as a baseline classifier for predicting wildfire occurrences using weather and environmental variables. Logistic regression is a simple yet powerful linear model that estimates the probability of a binary outcome—in this case, whether a wildfire occurs (`fire_occurred = 1`) or not (`fire_occurred = 0`).\n",
    "\n",
    "Because the wildfire dataset is often highly imbalanced, with far fewer wildfire events than non‑events, it’s important to account for this imbalance when training the model. In the following sections we load the data, perform basic cleaning, engineer features, split the data into training and testing sets, scale the features, train a logistic regression classifier with balanced class weights, and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0cbf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 01:19:17.497739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad521438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 13913 rows due to QC flags.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Update the file path to point to your data file\n",
    "data_path = './matched_data_20_miles.csv'\n",
    "\n",
    "# Load the matched data\n",
    "final_matched_data = pd.read_csv(data_path)\n",
    "\n",
    "# Identify quality control (QC) columns and remove rows with unreliable flags\n",
    "qc_columns = ['qc.1','qc.2','qc.3','qc.4','qc.5','qc.6','qc.7','qc.8','qc.9','qc.10','qc.11','qc.12','qc.13']\n",
    "excluded_flags = {'M','I','S'}  # Missing, Invalid, or suspect readings\n",
    "\n",
    "qc_data = final_matched_data[qc_columns].astype(str)\n",
    "\n",
    "def contains_excluded_flag(series, excluded_flags):\n",
    "    return series.apply(lambda val: any(flag in val for flag in excluded_flags))\n",
    "\n",
    "# Create a mask for rows without excluded QC flags\n",
    "column_masks = qc_data.apply(lambda col: ~contains_excluded_flag(col, excluded_flags))\n",
    "mask = column_masks.all(axis=1)\n",
    "\n",
    "# Filter out poor‑quality rows\n",
    "cleaned_data = final_matched_data[mask].copy()\n",
    "print(f'Removed {(~mask).sum()} rows due to QC flags.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a37ccc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (183666, 14)\n"
     ]
    }
   ],
   "source": [
    "# List the base feature columns\n",
    "general_features = [\n",
    "    'ETo (mm)', 'Precip (mm)', 'Sol Rad (W/sq.m)', 'Avg Vap Pres (kPa)',\n",
    "    'Max Air Temp (C)', 'Min Air Temp (C)', 'Avg Air Temp (C)',\n",
    "    'Max Rel Hum (%)', 'Min Rel Hum (%)', 'Avg Rel Hum (%)',\n",
    "    'Dew Point (C)', 'Avg Wind Speed (m/s)', 'Wind Run (km)', 'Avg Soil Temp (C)'\n",
    "]\n",
    "\n",
    "# Compute a 7‑day trailing average for each feature within each weather station\n",
    "for col in general_features:\n",
    "    cleaned_data[f'{col}_7d_avg'] = (\n",
    "        cleaned_data.groupby('StationNbr')[col]\n",
    "        .transform(lambda x: x.rolling(7, min_periods=1).mean())\n",
    "    )\n",
    "\n",
    "# Select the trailing average features and the target label\n",
    "all_trailing_features = [f'{col}_7d_avg' for col in general_features]\n",
    "X = cleaned_data[all_trailing_features].dropna()\n",
    "y = cleaned_data.loc[X.index, 'fire_occurred']\n",
    "\n",
    "print(f'Feature matrix shape: {X.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dbc8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize features using z‑score scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d4782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(1, input_shape=(X_train_scaled.shape[1],), activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e49cea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b0b56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4592/4592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.8748 - loss: 0.4229\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_scaled, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f6c21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.99\n",
      "\u001b[1m1148/1148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 890us/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Predict class labels\n",
    "y_probs = model.predict(X_test_scaled)\n",
    "y_pred = (y_probs >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35555cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "\n",
      "Full Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     36298\n",
      "           1       0.00      0.00      0.00       436\n",
      "\n",
      "    accuracy                           0.99     36734\n",
      "   macro avg       0.49      0.50      0.50     36734\n",
      "weighted avg       0.98      0.99      0.98     36734\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Report precision, recall, accuracy\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(\"\\nFull Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf3f9d1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a straightforward logistic regression baseline for wildfire prediction. We cleaned the raw matched dataset by removing rows with unreliable quality‑control flags, engineered 7‑day trailing averages for key weather features, and trained a logistic regression classifier with balanced class weights to handle the imbalanced target distribution.\n",
    "\n",
    "    •\tLogistic Regression:\n",
    "Accuracy = 99%, Precision = 0% for the wildfire class, Recall = 0%\n",
    "\n",
    "The accuracy rate is extremely high because of the class imbalance. The model only succesfully predict the majority 'no fire' class and fails to detect any fire events. \n",
    "\n",
    "Compare these results to other models such as Random Forests, gradient boosting (e.g., XGBoost), or deep neural networks. LR offers an interpretable baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c40f31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
